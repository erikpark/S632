---
title: "S632 Takehome Final"
author: "Erik Parker"
date: "April 24, 2018"
output: pdf_document
geometry: left=1.8cm,right=1.8cm,top=1.8cm,bottom=1.8cm
---

```{r setup, include=FALSE, message=FALSE, warning=FALSE, tidy=TRUE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, tidy = TRUE)
```

##### 1.

With the exception of the class instructor, I have not had any form of communication about this exam with any other individual (including other students, teaching assistants, instructors, etc.)  
Signed: Erik Parker  
If we take into account the low sample size of some of the schools which leads to some odd behavior such as in the scatterplot of *cses* vs *test* for school 182 where all students have the same *cses*, the scatterplots of centered SES and centered IQ against Test score for the 10 schools randomly selected do appear to be quite reasonably linear. In general, it seems that there is a relationship where as centered IQ and centered SES increase, so too does test score. The strength of this relationship seems to vary between the 10 schools displayed here, but its general presence is consistent in these data.


##### 2.

First for the plots of the intercepts, when we plot them against *class.size* the result closely resembles a null plot meaning that as the size of a class changes, there is no systematic change in the test scores of students in said class. When these intercepts are plotted against *meanses* and *meaniq* though, we see that there is a strong systematic trend, where classrooms of students with higher average iq's and ses tend to have higher average test scores as indicated here by higher intercepts. However, this relationship is exactly what we should expect to see given how the regression was constructed Recall that student's test scores were regressed against centered ses and centered IQ, which were calculated as the difference between an individual student's IQ and ses and the school wide averages of those two measures. Also recall that the intercept of a standard linear regression represents the expected value of the response (here *test*) when all the regressors (here *ciq* and *cses* are zero). This means that the intercepts being plotted here were obtained when the centered scores were zero, or equivalently, when an individual student's ses/IQ scores were the same as the school mean IQ/ses scores. So then it is no surprise that there is a linear relationship between *meanses* and *meaniq* and the intercepts, as the intercepts being plotted are exactly based on the mean ses and IQ scores for each school.  
Moving on to the six plots of the coefficients for *cses* and *ciq* against *class.size*, *meanses* and *meaniq*, we see that there is no clear systematic variation to any of them - all six of these slope coefficient plots more or less resemble null plots. Taken together with the previous three plots, this suggests that there is no systematic relationship between the class size, or the mean ses or IQ scores, and the average test scores of a school (intercept) or how those test scores might change with varying levels of centered ses and IQ scores of the students in said class (slope coefficients).


##### 3.

###### a)

When we fit a model with test scores as the response, no fixed effects, and only a random intercept for *school*, we see that the proportion of total variation in test scores among students just being due to the school they attend is 0.2269, or 22.69%.

###### b)

After testing the various random effects introduced into the model, we see that the random effects for students' centered IQ and an intercept, when grouped by school, are significant. Centered SES was found to be non-significant using the exact restricted likelihood ratio test. The models tested were constructed so that there was a covariance between centered IQ and centered ses because a plot of *cses* vs *ciq* for 10 random schools seemed to demonstrate that there was the possibility of a positive relationship between the two metrics, so I did not feel comfortable assuming they were independent.  
These tests were done on models containing fixed effects for individual student IQ scores, and ses as it intuitively seems very likely that those two regressors will contribute to the test scores of students - and the plots of centered IQ and SES from part 1 suggest that there is a relationship between these two variables (in some form) and test score. Raw IQ and SES were chosen over the centered scores because those did not strike me as wholly "student level" effects, as they are scaled by the school average. The assumption that these were valuable variables to choose will be further validated later in part d.  
From the summary of the model, we see that for the fixed effects: as a student's IQ and ses increase by one unit, their test scores increase by 2.30 and 0.17 points, respectively while holding the other regressor constant. The large difference between these two coefficient estimates is likely largely due to the difference in scale between the two variables (4-18.5 for *IQ* and 10-50 for *SES*), and so the real difference in effect between the two is not so large. For the random effects: we see first from the summary that the estimated inter-school variation in test scores is quite high (a SD of 3.08 against the residual SD of only 6.09), but that the variation based on *ciq* when grouped by school is quite small (SD of 0.48). This conclusion is further supported by plots of the 95% confidence intervals for each of these random effects, for each school which show that many of the intercepts are quite far from zero, while none of the *ciq* points are. This tells us that while *ciq* might have been tested as a random effect significantly different from zero using the exact test, the size of its effect is small.  

###### c)

Both of the random effects retained in the model from part b are both still significant when tested using the exact restricted likelihood ratio test, even after the additional school level predictors are added. That is, the test used returns low p-values, allowing us to reject the null hypotheses that the variance of the random effects tested (both centered IQ and an intercept grouped by school) is equal to zero.

###### d)

After a long string of Kenward-Roger approximate F-tests, we see that we are first able to drop the main effect for class size, and then that we are able to drop a number of interactions. In the end though, some higher level interactions were still seen to be significant. The eventual model arrived at contains a three way interaction between *IQ*, *SES*, and *meanIQ* (and the associated main effects and two way interactions), as well as a two way interaction between *meanSES*, and *meanIQ* (and the main effect for *meanSES*). Importantly, centered IQ and ses were only tested as fixed effects in a model by themselves (and an interaction between them) because they are linear combinations of the raw and mean IQ/ses scores, so they can't be added to a model containing the others. Also, the centered scores alone provide a reduced amount of information compared to the raw and mean scores together, so I felt comfortable choosing to not use them. This model was also chosen after pursuing a separate, AIC based approach for model selection.

###### e)

This model is quite complicated, so interpretations of the results will take some time. We see from diagnostics of the model first that the residuals are normal from the QQ plot and second that the variance is relatively constant across the range of fitted values, from the residual-fit plot. From this second plot we do see that the variance decreases at higher fitted values - this makes sense though because people who do well on tests generally do well at the same "level", because there's only so high one can score on a test.  
With these diagnostics completed, we see from the summary of the model and, more importantly, from the effect plots of the model - that in general test scores increase with the student level measures of IQ and ses, and also with school-wide mean ses and IQ measures, though the school-wide measures are a little more complicated and will be addressed specifically later. Almost all of the plots show that in this model, test scores increase with whatever x-variable they are tested against, and across any faceting variable that we consider.  
From these plots, we can also identify a few interesting trends. 1) When we "facet" by IQ and plot any of the other effects it interacts with, we see that students with higher iq's score higher on their tests on average compared to students with lower iq's. That is to say, students with higher iq's have a distribution of test scores concentrated near the high end of the spectrum, with relatively little overlap with students at the lower end of the IQ spectrum. 2) Mean IQ and mean SES have an interesting interactive relationship. When we plot the effect of mean SES, faceted by mean IQ we see that when the mean IQ of a school is low, an increase in the mean SES of a school leads to a modest increase in test scores. However, when the mean IQ of a school is high or intermediate, an increase in mean SES leads to a decrease in test scores. Similarly, when we examine the effect of mean IQ on test scores, when faceted by mean SES we see that test scores increase most with increases of mean IQ at low levels of mean SES and that the magnitude of this relationship decreases as mean SES increases. Together, these plots seems to suggest that students who attend schools with either very high mean SES or IQ scores generally tend to score high on their end of year tests, and increases in the other mean metric don't lead to the same magnitude of test score increases seen in schools with lower average SES and IQ scores.
For the random effects, we see that in this model both random effects are significant by an exact restricted likelihood ratio test. Furthermore, from the summary, we see that the standard deviation of the *ciq* random effect is 0.44, so while it is statistically significant in the model - it has a relatively low magnitude and so can be considered not overly important. The random intercept though has a standard deviation of 2.85 and so is quite important. 


\newpage

## Appendix

-----

# 1.

```{r}

rm(list=ls())
library(lme4)
library(alr4)
library(ggplot2)
library(faraway)
library(tidyr)
library(dplyr)
library(GGally)

school <- read.table("S18S632final.txt")

school <- school[complete.cases(school[,1:7]),]

school$cses <- school$ses-school$meanses
school$ciq <- school$iq-school$meaniq

set.seed(0330)

school.plot.ids <- sample(unique(school$school), 10)

school.plot <- school[school$school %in% school.plot.ids,]

ggplot(school.plot, aes(x = cses, y = test)) + geom_point() + facet_wrap(~school, ncol = 3) + labs(title = "Centered SES vs Test score by School", x = "Centered SES", y = "Test score")

ggplot(school.plot, aes(x = ciq, y = test)) + geom_point() + facet_wrap(~school, ncol = 3) + labs(title = "Centered IQ vs Test score by School", x = "Centered IQ", y = "Test score")

```

-----

\newpage

# 2. 

```{r}


library(broom)
library(gridExtra)

by_school <- group_by(school, school)

m1 <- do(by_school, tidy(lm(test ~ cses + ciq, data = .)))

m1 <- m1[,-c(4:6)]

m1 <- spread(m1, term, estimate)

others <- sample_n(by_school,1)
others <- others[,c(1,5:7)]

m1.final <- merge(m1,others,by = "school")
colnames(m1.final)[4] <- "intercept"

int.1 <- ggplot(m1.final, aes(x = class.size, y = intercept)) + geom_point()
int.2 <- ggplot(m1.final, aes(x = meanses, y = intercept)) + geom_point() + labs(y="")
int.3 <- ggplot(m1.final, aes(x = meaniq, y = intercept)) + geom_point() + labs(y="")
grid.arrange(int.1,int.2,int.3, ncol = 3)

ses.1 <- ggplot(m1.final, aes(x = class.size, y = cses)) + geom_point()
ses.2 <- ggplot(m1.final, aes(x = meanses, y = cses)) + geom_point() + labs(y="")
ses.3 <- ggplot(m1.final, aes(x = meaniq, y = cses)) + geom_point() + labs(y="")
grid.arrange(ses.1,ses.2,ses.3, ncol = 3)

iq.1 <- ggplot(m1.final, aes(x = class.size, y = ciq)) + geom_point()
iq.2 <- ggplot(m1.final, aes(x = meanses, y = ciq)) + geom_point() + labs(y="")
iq.3 <- ggplot(m1.final, aes(x = meaniq, y = ciq)) + geom_point() + labs(y="")
grid.arrange(iq.1,iq.2,iq.3, ncol = 3)

```

-----

\newpage

# 3. a)

```{r}

m.randint <- lmer(test ~ (1|school), school)

summary(m.randint)$var

intraclass.correlation <- (4.2743^2/(4.2743^2+7.8909^2))
intraclass.correlation

```
-----

\newpage

# 3. b) 


```{r}

library(RLRsim)
library(lattice)

ggplot(school.plot, aes(x = ciq, y = cses)) + geom_point() + facet_wrap(~school, ncol = 3) + labs(title = "Centered IQ vs Centered SES by School", x = "Centered IQ", y = "Centered SES")

m.5 <- lmer(test ~ iq + ses + (cses - 1|school), school)

m.6 <- lmer(test ~ iq + ses + (ciq - 1|school), school)

m.7 <- lmer(test ~ iq + ses + (1|school), school)

m.8 <- lmer(test ~ iq + ses + (1 + cses + ciq||school), school)

m.9 <- lmer(test ~ iq + ses + (1 + cses||school), school)

m.10 <- lmer(test ~ iq + ses + (1 + ciq||school), school)

m.11 <- lmer(test ~ iq + ses + (cses + ciq - 1||school), school)


# testing cses random effect

exactRLRT(m.5,m.8,m.10)

# testing ciq random effect

exactRLRT(m.6,m.8,m.9)

# testing school random effect intercept alone

exactRLRT(m.7,m.8,m.11)

sumary(m.10)

dotplot(ranef(m.10, condVar = TRUE))

```

-----

\newpage

# 3. c)


```{r}

m.c <- lmer(test ~ iq + ses + class.size + meanses + meaniq + (ciq - 1|school), school)

m.c2 <- lmer(test ~ iq + ses + class.size + meanses + meaniq + (ciq + 1||school), school)

m.c3 <- lmer(test ~ iq + ses + class.size + meanses + meaniq + (1|school), school)

# Test for ciq effect
exactRLRT(m.c,m.c2,m.c3)

# Test for intercept
exactRLRT(m.c3,m.c2,m.c)

```

-----

\newpage

3. d)

```{r}

library(pbkrtest)

m.main <- lmer(test ~ iq + ses + class.size + meanses + meaniq + (ciq + 1||school), school, REML = FALSE)

m.int <- lmer(test ~ iq * ses * class.size * meanses * meaniq + (ciq + 1||school), school, REML = FALSE)

KRmodcomp(m.int,m.main)
# Reject null hypothesis that reduced model explains data well enough, but not by much so likely need to simplify model a bit.

m.int2 <- lmer(test ~ iq * ses + class.size + meanses * meaniq + (ciq + 1||school), school, REML = FALSE)

KRmodcomp(m.int,m.int2)
# Can't reject null hypothesis that reduced model is sufficient. So remove interaction with class size.

m.int3 <- lmer(test ~ iq * ses + meanses * meaniq + (ciq + 1||school), school, REML = FALSE)

KRmodcomp(m.int2,m.int3)
# Can't reject null that reduced is sufficient, again. So remove class size.

m.int4 <- lmer(test ~ iq * ses + (ciq + 1||school), school, REML = FALSE)

KRmodcomp(m.int3,m.int4)
# This null model is rejected, so can't remove interaction between meanses and meaniq yet.

m.int5 <- lmer(test ~ meaniq * meanses + (ciq + 1||school), school, REML = FALSE)

KRmodcomp(m.int3,m.int5)
# This null model also rejected, can't remove student level iq and ses yet

m.int6 <- lmer(test ~ iq * ses + meanses + meaniq + (ciq + 1||school), school, REML = FALSE)

KRmodcomp(m.int3,m.int6)
# Null model rejected, so can't remove interaction between meanses and meaniq.

m.int7 <- lmer(test ~ iq + ses + meanses * meaniq + (ciq + 1||school), school, REML = FALSE)

KRmodcomp(m.int3,m.int7)
# Null model rejected again, so can't remove interaction between student level iq and ses.

m.int8 <- lmer(test ~ iq * ses * meanses * meaniq + (ciq + 1||school), school, REML = FALSE)

KRmodcomp(m.int8,m.int3)
# So reject null model, barely at alpha 0.05. Need to keep higher order interaction terms for now.

Anova(m.int8)
# Suggests that threeway interaction between iq, ses, and meaniq is needed.

m.int9 <- lmer(test ~ iq * ses * meaniq + meanses:meaniq + (ciq + 1||school), school, REML = FALSE)

KRmodcomp(m.int8,m.int9)
# Marginal, but can't reject null model. So looks like removal of 4 way interaction, and three ways with meanses is okay.

m.int10 <- lmer(test ~ iq * ses * meaniq + meanses:meaniq + class.size + (ciq + 1||school), school, REML = FALSE)

m.int11 <- lmer(test ~ iq * ses * meaniq + meanses:meaniq + meanses + (ciq + 1||school), school, REML = FALSE)

m.int12 <- lmer(test ~ ciq*cses + (ciq + 1||school), school, REML = FALSE)

KRmodcomp(m.int11,m.int9)
# Seems to be the best model, basically model 9 with addition of main effect for meanses.

# Now trying using AIC for model selection
anova(m.main,m.int,m.int2,m.int3,m.int4,m.int5,m.int6,m.int7,m.int8,m.int9,m.int10, m.int11,m.int12)[,1:4]
# The AIC approach also leads us to select m.int11, as it has the lowest value.

```

-----

\newpage

3. e) 

```{r}

library(visreg)

m.final <- lmer(test ~ iq * ses * meaniq + meanses:meaniq + meanses + (ciq + 1||school), school)

diagd <- fortify(m.final)
ggplot(diagd, aes(sample=.resid))+stat_qq()
# QQ plot looks great, residuals are normal.
ggplot(diagd, aes(x=.fitted,y=.resid)) + geom_point(alpha=0.3) + geom_hline(yintercept=0) + xlab("Fitted") + ylab("Residuals")
# Overall the residual-fit plot looks quite good. See that there is a reduction in the variance at higher fitted score values, which makes sense - people who do well on test generally do well at the same level, becuase there's only so high one can score on a test.

sumary(m.final)

visreg(m.final, "meaniq", by = "meanses", gg = TRUE)

visreg(m.final, "meanses", by = "meaniq", gg = TRUE)

visreg(m.final, "ses", by = "iq", gg = TRUE)

visreg(m.final, "ses", by = "meaniq", gg = TRUE)

visreg(m.final, "iq", by = "ses", gg = TRUE)

visreg(m.final, "meaniq", by = "ses", gg = TRUE)

visreg(m.final, "meaniq", by = "iq", gg = TRUE)

m.ciq <- lmer(test ~ iq * ses * meaniq + meanses:meaniq + meanses + (ciq - 1|school), school)

m.1rand <- lmer(test ~ iq * ses * meaniq + meanses:meaniq + meanses + (1|school), school)

exactRLRT(m.ciq,m.final,m.1rand)

exactRLRT(m.1rand,m.final,m.ciq)

```

-----

\newpage

## References

-----

1. Randomly select groups (and all cases per group) in R? (n.d.). Retrieved April 24, 2018, from https://stackoverflow.com/questions/13214769/randomly-select-groups-and-all-cases-per-group-in-r