knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, tidy = TRUE)
rm(list=ls())
library(alr4)
library(ggplot2)
library(faraway)
library(pscl)
library(GGally)
weed <- read.table("takehome.txt")
summary(weed)
# So, ranges of days and moist are quite large and both include zero. Also, lots of 0 counts of weeds. Probably should consider using a zero inflated poisson model.
ggpairs(weed)
# Moist looks almost bimodal, the lack of values from 52-77 is clear, but is it important?
weed$moistcat <- ifelse(weed$Moist < "52", "low", "high")
m1 <- glm(Weeds ~ ., data = weed, family = poisson)
m2 <- glm(Weeds ~ Moist*Days, data = weed, family = poisson)
m3 <- glm(Weeds ~ Moist, data = weed, family = poisson)
#m4 <- glm.nb(Weeds ~ Moist*Days, data = weed)
#m5 <- glm.nb(Weeds ~ Moist, data = weed)
m6 <- glm(Weeds ~ moistcat, data = weed, family = poisson)
m7 <- glm(Weeds ~ Moist + I(Moist^2), data = weed, family = poisson)
pchisq(deviance(m1),df.residual(m1),lower=FALSE)
pchisq(deviance(m2),df.residual(m2),lower=FALSE)
pchisq(deviance(m3),df.residual(m3),lower=FALSE)
pchisq(deviance(m6),df.residual(m6),lower=FALSE)
pchisq(deviance(m7),df.residual(m7),lower=FALSE)
pchisq(deviance(m3)-deviance(m1),df.residual(m3)-df.residual(m1),lower=FALSE)
pchisq(deviance(m1)-deviance(m2),df.residual(m1)-df.residual(m2),lower=FALSE)
pchisq(deviance(m3)-deviance(m2),df.residual(m3)-df.residual(m2),lower=FALSE)
pchisq(deviance(m6)-deviance(m3),df.residual(m6)-df.residual(m3),lower=FALSE)
pchisq(deviance(m3)-deviance(m7),df.residual(m3)-df.residual(m7),lower=FALSE)
# So, none of these models appear to fit particularly well, really low p-values from the goodness of fit tests. But, of the three models, we see that the one with only Moist (m3) seems to be the best, according to these tests. Additionally, we see from the summary command that m3 has the lowest AIC of these models.
# Also, from the fourth test above we can reject the null hypothesis that the reduced model, with our factor *moistcat*, fits better than the model with the full *Moist* regressor.
# Also, the addition of the quadratic term for Moist^2 does not seem to significantly improve the model, as seen in the fifth test above versus the reduced model with just *Moist*.
# So, of all the basic poisson models, we see that the model with *Moist* alone fits the best.
# Will now try some tests to see why the deviance is still so high in m3.
pcount <- colSums(predprob(m3)[,1:13])
ocount <- table(weed$Weeds)[1:13]
plot(pcount,ocount,type="n",xlab="Predicted",ylab="Observed")
text(pcount,ocount, 0:12)
abline(0,1)
# The observed number of zeros (17) is much higher than the expected (~13), so it seems like going with a zero inflated poisson model would be a good choice here. But, the number of 1's seems to be higher than expected as well. Maybe the probability of observing 0 or 1 type of weeds is different than the other amounts and so 1 should be the hurdle?
mz1 <- hurdle(formula = Weeds ~ Moist, data = weed)
mz2 <- hurdle(formula = Weeds ~ Moist + Days, data = weed)
#mz3 <- zeroinfl(formula = Weeds ~ Moist, data = weed)
mz4 <- hurdle(formula = Weeds ~ Moist, data = weed, level = 1)
lrt <- 2*(mz2$loglik - mz1$loglik)
1-pchisq(lrt, 2)
# So, can't reject null hypothesis that the reduced model, mz1, adaquately explains the response.
pcount <- colSums(predprob(mz1)[,1:13])
plot(pcount,ocount,type="n",xlab="Predicted",ylab="Observed")
text(pcount,ocount, 0:12)
abline(0,1)
# So fitting the hurdle model makes this look much better, but the 1 count still has more observations than predicted.
halfnorm(residuals(mz1))
# No outliers
plot(log(fitted(mz1)),log((weed$Weeds-fitted(mz1))^2),
xlab=expression(hat(mu)),ylab=expression((y-hat(mu))^2))
abline(0,1)
# Mean seems pretty close to the variance here, not perfect, but it's okay.
summary(mz1)
exp(coef(mz1))
newlawn <- data.frame(Moist = 83, Days = 365)
predict(mz1, newdata = newlawn, type = "response")
predict(mz1, newdata = newlawn, type = "prob")
# Also, can look at the probability coming from the "zero"" part of the model
# 1 - predict(mz1, newdata = newlawn, type = "zero")
# When the hurdle model is used, type = zero gives the probability of observing a non-zero count, based on the zero hurdle component - so this number is the probability of seeing a non-zero number of types of weeds. So 1- the predict command here is the probability of seeing a zero-number of types from the zero component of the hurdle model alone.
weedtable <- predict(mz1, newdata = newlawn, type = "prob")
sum(weedtable[5:14])
X = model.matrix(Weeds ~ Days*Moist, weed)
y = as.numeric(weed$Weeds)
beta_t = rep(0,4)
mu = exp(X%*%beta_t)
beta_t1 = beta_t + solve(t(X)%*%diag(c(mu))%*%X)%*%t(X)%*%(y-mu)
i.count = 1
while (sum((beta_t1-beta_t)^2) > 1e-6){
beta_t = beta_t1
mu = exp(X%*%beta_t)
beta_t1 = beta_t + solve(t(X)%*%diag(c(mu))%*%X)%*%t(X)%*%(y-mu)
i.count = i.count+1
print(c(i.count,beta_t1))
}
beta_t1
m.rep <- glm(Weeds ~ Days*Moist, family = "poisson", data = weed)
summary(m.rep)$coef[,1]
summary(m.rep)$coef
beta_t1
summary(m.rep)$coef[,1]
beta_t1
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, tidy = TRUE)
rm(list=ls())
library(lme4)
library(alr4)
library(ggplot2)
library(faraway)
library(tidyr)
library(dplyr)
library(GGally)
school <- read.table("S18S632final.txt")
school <- school[complete.cases(school[,1:7]),]
school$cses <- school$ses-school$meanses
school$ciq <- school$iq-school$meaniq
set.seed(0330)
school.plot.ids <- sample(unique(school$school), 10)
school.plot <- school[school$school %in% school.plot.ids,]
ggplot(school.plot, aes(x = cses, y = test)) + geom_point() + facet_wrap(~school, ncol = 3) + labs(title = "Centered SES vs Test score by School", x = "Centered SES", y = "Test score")
ggplot(school.plot, aes(x = ciq, y = test)) + geom_point() + facet_wrap(~school, ncol = 3) + labs(title = "Centered IQ vs Test score by School", x = "Centered IQ", y = "Test score")
library(broom)
library(gridExtra)
by_school <- group_by(school, school)
m1 <- do(by_school, tidy(lm(test ~ cses + ciq, data = .)))
m1 <- m1[,-c(4:6)]
m1 <- spread(m1, term, estimate)
others <- sample_n(by_school,1)
others <- others[,c(1,5:7)]
m1.final <- merge(m1,others,by = "school")
colnames(m1.final)[4] <- "intercept"
int.1 <- ggplot(m1.final, aes(x = class.size, y = intercept)) + geom_point()
int.2 <- ggplot(m1.final, aes(x = meanses, y = intercept)) + geom_point() + labs(y="")
int.3 <- ggplot(m1.final, aes(x = meaniq, y = intercept)) + geom_point() + labs(y="")
grid.arrange(int.1,int.2,int.3, ncol = 3)
ses.1 <- ggplot(m1.final, aes(x = class.size, y = cses)) + geom_point()
ses.2 <- ggplot(m1.final, aes(x = meanses, y = cses)) + geom_point() + labs(y="")
ses.3 <- ggplot(m1.final, aes(x = meaniq, y = cses)) + geom_point() + labs(y="")
grid.arrange(ses.1,ses.2,ses.3, ncol = 3)
iq.1 <- ggplot(m1.final, aes(x = class.size, y = ciq)) + geom_point()
iq.2 <- ggplot(m1.final, aes(x = meanses, y = ciq)) + geom_point() + labs(y="")
iq.3 <- ggplot(m1.final, aes(x = meaniq, y = ciq)) + geom_point() + labs(y="")
grid.arrange(iq.1,iq.2,iq.3, ncol = 3)
m.randint <- lmer(test ~ (1|school), school)
summary(m.randint)$var
intraclass.correlation <- (4.2743^2/(4.2743^2+7.8909^2))
intraclass.correlation
m.randint <- lmer(test ~ (1|school), school)
summary(m.randint)$var
intraclass.correlation <- (4.2743^2/(4.2743^2+7.8909^2))
intraclass.correlation
library(RLRsim)
library(lattice)
m.5 <- lmer(test ~ iq + ses + (cses - 1|school), school)
m.6 <- lmer(test ~ iq + ses + (ciq - 1|school), school)
m.7 <- lmer(test ~ iq + ses + (1|school), school)
m.8 <- lmer(test ~ iq + ses + (1 + cses + ciq||school), school)
m.9 <- lmer(test ~ iq + ses + (1 + cses||school), school)
m.10 <- lmer(test ~ iq + ses + (1 + ciq||school), school)
m.11 <- lmer(test ~ iq + ses + (cses - 1 + ciq - 1||school), school)
exactRLRT(m.5,m.8,m.10)
exactRLRT(m.6,m.8,m.9)
m.10 <- lmer(test ~ iq + ses + (ciq|school), school)
exactRLRT(m.5,m.8,m.10)
m.10 <- lmer(test ~ iq + ses + (1 + ciq||school), school)
sumary(m.11)
m.11 <- lmer(test ~ iq + ses + (cses + ciq - 1||school), school)
sumary(m.11)
exactRLRT(m.6,m.8,m.9)
exactRLRT(m.7,m.8,m.11)
sumary(m.10)
.48^2
dotplot(ranef(m.10, condVar = TRUE))
m.c <- lmer(test ~ iq + ses + class.size + meanses + meaniq + (ciq - 1|school), school)
m.c2 <- lmer(test ~ iq + ses + class.size + meanses + meaniq + (ciq + 1||school), school)
m.c3 <- lmer(test ~ iq + ses + class.size + meanses + meaniq + (1|school), school)
# Test for ciq effect
exactRLRT(m.c,m.c2,m.c3)
# Test for intercept
exactRLRT(m.c3,m.c2,m.c)
library(pbkrtest)
m.main <- lmer(test ~ iq + ses + class.size + meanses + meaniq + (ciq + 1||school), school, REML = FALSE)
m.int <- lmer(test ~ iq * ses * class.size * meanses * meaniq + (ciq + 1||school), school, REML = FALSE)
KRmodcomp(m.int,m.main)
?KRmodcomp
m.int2 <- lmer(test ~ iq * ses + class.size + meanses * meaniq + (ciq + 1||school), school, REML = FALSE)
KRmodcomp(m.int,m.int2)
m.int8 <- lmer(test ~ iq * ses * meanses * meaniq + (ciq + 1||school), school, REML = FALSE)
Anova(m.int8)
m.int10 <- lmer(test ~ iq * ses * meaniq + meanses:meaniq + class.size + (ciq + 1||school), school, REML = FALSE)
m.int11 <- lmer(test ~ iq * ses * meaniq + meanses:meaniq + meanses + (ciq + 1||school), school, REML = FALSE)
m.int12 <- lmer(test ~ ciq*cses + (ciq + 1||school), school, REML = FALSE)
m.int9 <- lmer(test ~ iq * ses * meaniq + meanses:meaniq + (ciq + 1||school), school, REML = FALSE)
m.int6 <- lmer(test ~ iq * ses + meanses + meaniq + (ciq + 1||school), school, REML = FALSE)
m.int7 <- lmer(test ~ iq + ses + meanses * meaniq + (ciq + 1||school), school, REML = FALSE)
m.int5 <- lmer(test ~ meaniq * meanses + (ciq + 1||school), school, REML = FALSE)
m.int4 <- lmer(test ~ iq * ses + (ciq + 1||school), school, REML = FALSE)
m.int3 <- lmer(test ~ iq * ses + meanses * meaniq + (ciq + 1||school), school, REML = FALSE)
m.int2 <- lmer(test ~ iq * ses + class.size + meanses * meaniq + (ciq + 1||school), school, REML = FALSE)
# Now trying using AIC for model selection
anova(m.main,m.int,m.int2,m.int3,m.int4,m.int5,m.int6,m.int7,m.int8,m.int9,m.int10, m.int11,m.int12)[,1:4]
library(visreg)
m.final <- lmer(test ~ iq * ses * meaniq + meanses:meaniq + meanses + (ciq + 1||school), school)
diagd <- fortify(m.final)
ggplot(diagd, aes(sample=.resid))+stat_qq()
# QQ plot looks great, residuals are normal.
ggplot(diagd, aes(x=.fitted,y=.resid)) + geom_point(alpha=0.3) + geom_hline(yintercept=0) + xlab("Fitted") + ylab("Residuals")
visreg(m.final, "iq", by = "meaniq","ses", gg = TRUE)
?lmlist
??lmlist
?lmList
